Hereâ€™s a **complete guide using AWS Console (no Terraform/CLI)** to implement the serverless architecture:
ğŸ“¦ **S3 â†’ SQS â†’ Lambda â†’ DLQ + CloudWatch + SNS**.

---

## âœ… High-Level Architecture

```plaintext
User Uploads File to S3
        â†“
S3 Event Notification â†’ SQS Queue
        â†“
Lambda polls SQS, processes object
        â†“
Failure? â†’ DLQ â†’ CloudWatch Alarm â†’ SNS Email Alert
```
![image](https://github.com/user-attachments/assets/4e4fa539-6e5b-421f-bf88-759a16744204)

---

## ğŸ“˜ Step-by-Step Setup Using AWS Console

---

### ğŸ”¹ Step 1: Create S3 Bucket

1. Go to **S3** â†’ **Create bucket**
2. Bucket name: `s3-object-upload-bucket`
3. Disable â€œBlock all public accessâ€ (for simplicity in dev)
4. Create the bucket

---

### ğŸ”¹ Step 2: Create SQS Queues (Main + DLQ)

#### 2.1 Create DLQ

1. Go to **Amazon SQS** â†’ **Create queue**
2. Name: `s3-dlq`
3. Type: **Standard**
4. Leave rest default â†’ Create

#### 2.2 Create Main Queue

1. Go back to **SQS â†’ Create queue**
2. Name: `s3-event-queue`
3. Type: **Standard**
4. Scroll to **Dead-letter queue**

   * Enable DLQ
   * Select `s3-dlq`
   * Set maxReceiveCount = `2`
5. Create the queue

---

### ğŸ”¹ Step 3: Configure S3 to Send Events to SQS

1. Go to **S3 â†’ Your bucket â†’ Properties**
2. Scroll to **Event notifications** â†’ Create event
3. Name: `s3-to-sqs-event`
4. Event types: `PUT` (Object Created)
5. Destination: **SQS Queue**
6. Choose `s3-event-queue`
7. Save

---

### ğŸ”¹ Step 4: Create IAM Role for Lambda

1. Go to **IAM â†’ Roles â†’ Create role**
2. Trusted entity: **Lambda**
3. Add permissions:

   * `AmazonS3ReadOnlyAccess`
   * `AmazonSQSFullAccess`
   * `CloudWatchLogsFullAccess`
4. Name: `lambda-s3-sqs-role`
5. Create role

---

### ğŸ”¹ Step 5: Create Lambda Function

1. Go to **Lambda â†’ Create function**
2. Name: `s3-object-processor`
3. Runtime: **Python 3.9**
4. Execution role: Choose **Existing role** â†’ `lambda-s3-sqs-role`
5. Create function

#### Paste this code:

```python
import json
import logging
import boto3
from botocore.exceptions import ClientError

# Setup logging
logger = logging.getLogger()
logger.setLevel(logging.INFO)

# AWS clients
s3_client = boto3.client('s3')

def lambda_handler(event, context):
    logger.info("Lambda triggered with event: %s", json.dumps(event))

    for record in event['Records']:
        try:
            # Step 1: Parse SQS message
            message_body = json.loads(record['body'])
            s3_info = message_body['Records'][0]['s3']
            bucket_name = s3_info['bucket']['name']
            object_key = s3_info['object']['key']

            logger.info(f"New file uploaded to S3: {bucket_name}/{object_key}")

            # Step 2: Fetch object from S3
            file_obj = s3_client.get_object(Bucket=bucket_name, Key=object_key)
            file_content = file_obj['Body'].read().decode('utf-8')
            logger.info(f"Fetched file content: {file_content[:100]}...")  # Show only first 100 chars

            # Step 3: Business logic (e.g., parse JSON content)
            try:
                parsed_data = json.loads(file_content)
                logger.info("Parsed JSON content successfully")
                # Simulate further processing (e.g., validation)
                if 'id' not in parsed_data:
                    raise ValueError("Missing required 'id' field in file content")
                logger.info(f"Processed data with ID: {parsed_data['id']}")

            except json.JSONDecodeError as jde:
                logger.error("File content is not valid JSON.")
                raise jde
            except Exception as be:
                logger.error(f"Business logic error: {str(be)}")
                raise be

        except Exception as e:
            logger.error(f"Failed to process SQS message: {str(e)}")
            raise e  # Let Lambda rethrow to send message to DLQ
```

---

### ğŸ”¹ Step 6: Add Event Source Mapping (Trigger Lambda from SQS)

1. In Lambda â†’ `s3-object-processor` â†’ **Configuration**
2. Click **Add trigger** â†’ Choose **SQS**
3. Select queue: `s3-event-queue`
4. Batch size: `1`
5. Enable trigger

---

### ğŸ”¹ Step 7: Test the Flow

1. Upload any file to your S3 bucket.
2. Go to **CloudWatch Logs â†’ Log groups â†’ /aws/lambda/s3-object-processor**
3. Check logs to see Lambda has processed the message.

---

### ğŸ”¹ Step 8: CloudWatch Alarm for DLQ

1. Go to **CloudWatch â†’ Alarms â†’ Create Alarm**
2. Select metric:

   * Browse â†’ SQS â†’ **Per-Queue Metrics**
   * Choose `s3-dlq` â†’ `ApproximateNumberOfMessagesVisible`
3. Threshold: **Greater than 0**
4. Evaluation period: 1 of 1 minute
5. Next â†’ Create a new SNS topic (if not created)

---

### ğŸ”¹ Step 9: SNS Topic for Alerts

1. Create topic: `dlq-alert-topic`
2. Create subscription:

   * Protocol: **Email**
   * Endpoint: [your-email@example.com](mailto:your-email@example.com)
3. Confirm email subscription via inbox

---

## âœ… Youâ€™re Done!

### ğŸ”„ Test Failure Handling

---

## ğŸ§ª Sample Test File (Upload this to S3)

### `valid_sample.json`

```json
{
  "id": "12345",
  "name": "Deepak's Test File"
}
```

### `invalid_sample.json` (to test DLQ)

```json
{
  "name": "Missing ID"
}
```

---

## âœ… What This Lambda Does

| Step       | Logic                                              |
| ---------- | -------------------------------------------------- |
| âœ… Parse    | Reads the SQS message triggered by S3 upload       |
| âœ… Fetch    | Downloads the file content from S3                 |
| âœ… Validate | Parses JSON and checks for a required field (`id`) |
| âŒ Failures | If file is invalid, sends message to DLQ           |
| ğŸ“œ Logs    | Detailed logging via CloudWatch                    |

---

---

## ğŸ›¡ï¸ Key Benefits

* Fully **serverless** and **event-driven**
* Fault-tolerant with DLQ
* Real-time observability with CloudWatch
* Scalable and loosely coupled

---

